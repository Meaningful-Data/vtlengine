# VTL Engine - Claude Code Instructions

## Project Overview

VTL Engine is a Python library for validating, formatting, and executing VTL (Validation and Transformation Language) 2.1 scripts. It's built around ANTLR-generated parsers and uses Pandas DataFrames for data manipulation.

**VTL 2.1 Reference Manual**: <https://sdmx.org/wp-content/uploads/VTL-2.1-Reference-Manual.pdf>

## Core Architecture

### Parser Pipeline (ANTLR → AST → Interpreter)

1. **Lexing/Parsing** (`src/vtlengine/AST/Grammar/`): ANTLR4 grammar generates lexer/parser (DO NOT manually edit)
2. **AST Construction** (`src/vtlengine/AST/ASTConstructor.py`): Visitor pattern transforms parse tree to typed AST nodes
3. **Interpretation** (`src/vtlengine/Interpreter/__init__.py`): `InterpreterAnalyzer` walks AST and executes operations

To add new operators:

- Define AST node in `src/vtlengine/AST/__init__.py`
- Add visitor method in `ASTConstructor.py`
- Implement semantic analysis in `Interpreter/__init__.py`
- Add operator implementation in `src/vtlengine/Operators/`

### Data Model (`src/vtlengine/Model/__init__.py`)

- **Dataset**: Components (identifiers/attributes/measures) + Pandas DataFrame
- **Component**: Name, data_type, role (IDENTIFIER/ATTRIBUTE/MEASURE), nullable flag
- **Scalar**: Single-value results with type checking

Identifiers cannot be nullable; measures can. Role determines clause behavior.

### Type System (`src/vtlengine/DataTypes/`)

Hierarchy: `String`, `Number`, `Integer`, `Boolean`, `Date`, `TimePeriod`, `TimeInterval`, `Duration`, `Null`

All operators MUST validate types before execution.

## Public API (`src/vtlengine/API/__init__.py`)

- `run()`: Execute VTL script with data structures + datapoints
- `run_sdmx()`: SDMX-specific wrapper using `pysdmx.PandasDataset`
- `semantic_analysis()`: Validate script and infer output structures (no execution)
- `prettify()`: Format VTL scripts

## Testing

### Organization

- Each operator/feature has its own directory: `tests/Aggregate/`, `tests/Joins/`, etc.
- Test files: `test_*.py` extending `TestHelper` from `tests/Helper.py`
- Data files: `data/{vtl,DataStructure/input,DataSet/input,DataSet/output}/`

### Naming Convention

Test code `"1-1"` maps to:

- VTL script: `data/vtl/1-1.vtl`
- Input structure: `data/DataStructure/input/DS_1-1.json`
- Input data: `data/DataSet/input/DS_1-1.csv`
- Output reference: `data/DataSet/output/DS_r_1-1.csv`

### Running Tests

```bash
poetry run pytest
```

## Code Quality (mandatory before every commit)

```bash
poetry run ruff format
poetry run ruff check --fix --unsafe-fixes
poetry run mypy
```

### Ruff Rules

- Max line length: 100
- Max complexity: 20

### Mypy

- Strict mode for `src/` (except `src/vtlengine/AST/Grammar/` which is autogenerated)
- All functions MUST have type annotations
- No implicit optionals

## Error Handling

- **SemanticError**: Data structure/type compatibility issues (incompatible types, missing components, invalid roles)
- **RuntimeError**: Datapoints handling issues during execution (data conversion, computation errors)

## GitHub Project

**Open Source Initiatives**: <https://github.com/orgs/Meaningful-Data/projects/2>

Project ID: `PVT_kwDOA9gk5M4Aurey`

### Project Fields

Each issue in the project tracks the following fields:

| Field | Type | Values |
| ----- | ---- | ------ |
| Status | Single Select | Todo, In Progress, In Review, Awaiting for BIS Review, Done |
| Priority | Single Select | P0, P1, P2 |
| Size | Single Select | XS, S, M, L, XL |
| Estimate | Number | Hours estimate for the task |
| Iteration | Iteration | Current iterations (e.g., Iteration 28, 29) |
| Start date | Date | When work begins |
| End date | Date | Target completion |

### Querying the Project

```bash
# List all projects
gh api graphql -f query='
{
  organization(login: "Meaningful-Data") {
    projectsV2(first: 10) {
      nodes { id title number url }
    }
  }
}'

# Get project items with field values
gh api graphql -f query='
{
  organization(login: "Meaningful-Data") {
    projectV2(number: 2) {
      items(first: 20) {
        nodes {
          content {
            ... on Issue { number title state }
          }
          fieldValues(first: 10) {
            nodes {
              ... on ProjectV2ItemFieldSingleSelectValue {
                name
                field { ... on ProjectV2SingleSelectField { name } }
              }
              ... on ProjectV2ItemFieldNumberValue {
                number
                field { ... on ProjectV2Field { name } }
              }
            }
          }
        }
      }
    }
  }
}'
```

## Git Workflow

### Branch Naming

Pattern: `cr-{issue_number}` (e.g., `cr-457` for issue #457)

### Workflow

1. Create branch: `git checkout -b cr-{issue_number}`
2. Make changes with descriptive commits
3. Run all quality checks (ruff format, ruff check, mypy, pytest)
4. Push and create draft PR: `gh pr create --draft --title "Fix #{issue_number}: Description"`
5. Never add the PR to a milestone

### Issue Conventions

- Never include links to gitlab in issue descriptions
- Use issue types instead of labels: `Bug`, `Feature`, or `Task`
- Use standard dataset/component naming: `DS_1`, `DS_2` for datasets; `Id_1`, `Id_2` for identifiers; `Me_1`, `Me_2` for measures; `At_1`, `At_2` for attributes
- Always run the reproduction script to get the actual output — never guess or manually write it. If the output is data, format it as a markdown table for clarity
- Include a self-contained Python reproduction script using `run()` instead of separate VTL/JSON/CSV files:

```python
import pandas as pd
from vtlengine import run

script = """DS_r <- DS_1 * 10;"""

data_structures = {
    "datasets": [
        {
            "name": "DS_1",
            "DataStructure": [
                {"name": "Id_1", "type": "Integer", "role": "Identifier", "nullable": False},
                {"name": "Me_1", "type": "Number", "role": "Measure", "nullable": True},
            ],
        }
    ]
}

data_df = pd.DataFrame({"Id_1": [1, 2, 3], "Me_1": [10, 20, 30]})
datapoints = {"DS_1": data_df}

result = run(script=script, data_structures=data_structures, datapoints=datapoints)
print(result)
```

## Common Pitfalls

1. **Never edit Grammar files** - They're ANTLR-generated. Change `.g4` and regenerate if needed.
2. **Test data naming** - Code `"GL_123"` needs files `GL_123.vtl`, `DS_GL_123.json`, etc.
3. **AST node equality** - Override `ast_equality()` when adding nodes
4. **Nullable identifiers** - Will raise `SemanticError("0-1-1-13")`
5. **ANTLR version** - Must use 4.9.x to match `antlr4-python3-runtime` dependency
6. **Version updates** - When bumping version, update BOTH `pyproject.toml` AND `src/vtlengine/__init__.py`

## External Dependencies

- **pandas** (2.x): Dataset.data is a DataFrame
- **DuckDB** (1.4.x): Optional SQL engine for specific operations
- **pysdmx** (≥1.5.2): SDMX 3.0 data handling
- **sqlglot** (22.x): SQL parsing for external routines
- **antlr4-python3-runtime** (4.9.x): Parser runtime
