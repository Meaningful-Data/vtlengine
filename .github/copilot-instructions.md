# VTL Engine - AI Coding Agent Instructions

## Project Overview

VTL Engine is a Python library for validating, formatting, and executing VTL (Validation and Transformation Language) 2.1 scripts. It's built around ANTLR-generated parsers and uses Pandas DataFrames for data manipulation.

**VTL 2.1 Reference Manual**: https://sdmx.org/wp-content/uploads/VTL-2.1-Reference-Manual.pdf

## Core Architecture

### 1. Parser Pipeline (ANTLR → AST → Interpreter)

The execution flow follows a strict three-stage pattern:

1. **Lexing/Parsing** (`src/vtlengine/AST/Grammar/`): ANTLR4 grammar generates lexer/parser (DO NOT manually edit)
2. **AST Construction** (`src/vtlengine/AST/ASTConstructor.py`): Visitor pattern transforms parse tree to typed AST nodes
3. **Interpretation** (`src/vtlengine/Interpreter/__init__.py`): `InterpreterAnalyzer` walks AST and executes operations

**Key Pattern**: All AST visitors extend `ASTTemplate` (visitor base class with default traversal methods). To add new operators:
- Define AST node in `src/vtlengine/AST/__init__.py` 
- Add visitor method in `ASTConstructor.py` (parse tree → AST)
- Implement semantic analysis in `Interpreter/__init__.py`
- Add operator implementation in `src/vtlengine/Operators/`

### 2. Data Model (src/vtlengine/Model/__init__.py)

Three core data structures:
- **Dataset**: Components (identifiers/attributes/measures) + Pandas DataFrame
- **Component**: Name, data_type (from DataTypes), role (IDENTIFIER/ATTRIBUTE/MEASURE), nullable flag
- **Scalar**: Single-value results with type checking

**Critical**: Identifiers cannot be nullable; measures can. Role determines clause behavior (e.g., `calc` creates measures, not identifiers).

### 3. Type System (src/vtlengine/DataTypes/)

Strict hierarchy: `String`, `Number`, `Integer`, `Boolean`, `Date`, `TimePeriod`, `TimeInterval`, `Duration`, `Null`
- Type promotion rules in `check_unary_implicit_promotion()` and binary equivalents
- All operators MUST validate types before execution (see `Operators/*/validate()` pattern)

## Public API Entry Points

Main functions in `src/vtlengine/API/__init__.py`:
- `run()`: Execute VTL script with data structures + datapoints (CSV/DataFrame)
- `run_sdmx()`: SDMX-specific wrapper using `pysdmx.PandasDataset`
- `semantic_analysis()`: Validate script and infer output structures (no execution)
- `prettify()`: Format VTL scripts
- `validate_dataset()`, `validate_value_domain()`, `validate_external_routine()`: Input validation

**Common Pattern**: Scripts can be strings, Paths, or `TransformationScheme` objects. DAG analysis (`AST/DAG.py`) validates dependency graph before execution.

**Execution Lifecycle**:
1. `load_datasets_with_data()` loads data structures and datapoints
2. Data validation ensures datapoints match structures
3. AST traversal via `InterpreterAnalyzer` generates results for each transformation (AST children on Start node)
4. Each visit method returns evaluated result; inspect at return statements for debugging

## Testing Standards

### Test Organization (tests/)
- Each operator/feature has directory: `tests/Aggregate/`, `tests/Joins/`, etc.
- Files follow pattern: `test_*.py` with helper class extending `TestHelper`
- Data files: `data/{vtl,DataStructure/input,DataSet/input,DataSet/output}/`

### Test Helper Pattern (tests/Helper.py)
```python
class MyTest(TestHelper):
    base_path = Path(__file__).parent
    filepath_VTL = base_path / "data" / "vtl"
    filepath_json = base_path / "data" / "DataStructure" / "input"
    filepath_csv = base_path / "data" / "DataSet" / "input"
    
    def test_case(self):
        code = "1-1"  # References {code}.vtl, DS_{code}.json, DS_{code}.csv
        self.BaseTest(code=code, number_inputs=1, references_names=["DS_r"])
```

**Critical Convention**: Test code `"1-1"` automatically maps to:
- VTL script: `data/vtl/1-1.vtl`
- Input structure: `data/DataStructure/input/DS_1-1.json`
- Input data: `data/DataSet/input/DS_1-1.csv`
- Output reference: `data/DataSet/output/DS_r_1-1.csv`

Run tests: `pytest tests/` (uses `pytest-xdist` for parallelization)

## Code Quality Requirements

### Ruff Configuration (pyproject.toml)
- Max line length: 100 characters
- Max complexity: 20
- Key ignored rules: D* (most docstrings), S608 (DuckDB queries), B023/B028/B904
- Tests exempt from: S101 (asserts), PT006/PT012/PT013 (pytest styles)

### Mypy Type Checking
- Strict mode enabled for `src/` (except `src/vtlengine/AST/Grammar/` - autogenerated)
- All functions MUST have type annotations
- No implicit optionals

Run checks: `ruff check src/` and `mypy src/`

### Error Handling
- **SemanticError**: Data structure and data type compatibility issues within operators (e.g., incompatible types, missing components, invalid roles)
- **RuntimeError**: Datapoints handling issues during execution (e.g., data conversion failures, computation errors)
- Always raise appropriate error type based on whether issue is structural/semantic vs execution/runtime

## VTL-Specific Patterns

### Operator Implementation Template
Operators in `src/vtlengine/Operators/` follow standard structure:
```python
class MyOperator:
    @classmethod
    def validate(cls, left: Dataset, right: Any) -> Dataset:
        # 1. Type checking
        # 2. Component validation
        # 3. Return output Dataset structure (without data)
        pass
    
    @classmethod  
    def compute(cls, left: Dataset, right: Any, **kwargs) -> Dataset:
        # Manipulate Dataset.data (Pandas DataFrame) directly
        # DuckDB may be used for specific SQL operations when needed
        # Return Dataset with computed data
        pass
```

**Operator Organization**: Operators are grouped following the [VTL 2.1 Reference Manual](https://sdmx.org/wp-content/uploads/VTL-2.1-Reference-Manual.pdf) structure (Aggregate, Join, String, Numeric, etc.). Refer to the spec for type promotion rules and component mutation semantics.

### DAG Analysis
Before execution, `DAGAnalyzer.ds_structure(ast)` validates:
- No circular dependencies
- All referenced datasets exist
- Input/output dataset structures
- Determines computation order for transformations
- Identifies when datasets can be freed from memory after writing data

Access via: `dag_analysis = DAGAnalyzer.ds_structure(ast)` → `dag_analysis["global_inputs"]`

### Assignment Types
- `DS_A := expr;` - Temporary assignment (`:=`)
- `DS_A <- expr;` - Persistent assignment (`<-`, saved to output if provided)
- `return_only_persistent=True` (default) filters results

## Common Pitfalls

1. **Never edit Grammar files** - They're ANTLR-generated. Change `.g4` and regenerate if needed.
2. **Test data naming** - Code `"GL_123"` needs files `GL_123.vtl`, `DS_GL_123.json`, etc. (underscores matter!)
3. **AST node equality** - Override `ast_equality()` when adding nodes, don't rely on `__eq__`
4. **Nullable identifiers** - Will raise `SemanticError("0-1-1-13")` at data load time
5. **Time period formats** - Three output modes: `"vtl"`, `"sdmx_gregorian"`, `"sdmx_reporting"` (controlled by `time_period_output_format`)
6. **External routines scope** - Only executed in Eval operator, only on in-memory data (never external databases)
7. **Debugging operators** - Inspect operator returns at each `visit_*` method's return statement in `InterpreterAnalyzer` for step-by-step debugging

## ANTLR Grammar Regeneration

Grammar files are in `src/vtlengine/AST/Grammar/`:
- `Vtl.g4` - Main VTL grammar rules
- `VtlTokens.g4` - Token definitions
- Generated files: `lexer.py`, `parser.py`, `tokens.py` (DO NOT EDIT)

**Regeneration Steps** (requires ANTLR 4.9.x):
```bash
cd src/vtlengine/AST/Grammar

# Install ANTLR (if not available)
pip install antlr4-tools

# Regenerate parser from grammar
antlr4 -Dlanguage=Python3 -visitor Vtl.g4

# Verify regeneration
python -c "from vtlengine.AST.Grammar.parser import Parser; print('OK')"
```

**When to regenerate**:
- Adding new VTL operators or keywords
- Fixing parsing issues with specific VTL syntax
- Updating to match VTL 2.1 specification changes

**Critical**: Always use ANTLR version 4.9.x to match `antlr4-python3-runtime` dependency.

## SDMX 3.0 Integration

The VTL Engine integrates with SDMX 3.0 via `pysdmx` library for statistical data exchange.

### Core SDMX Concepts

| SDMX Concept | VTL Mapping | Notes |
|--------------|-------------|-------|
| `PandasDataset` | `Dataset` | Data + structure via `Schema` |
| `Schema` / `DataStructureDefinition` | Data structure JSON | Component definitions |
| `Dimension` | `Identifier` | Non-nullable by definition |
| `Measure` | `Measure` | Nullable |
| `Attribute` | `Attribute` | Nullable |
| `TransformationScheme` | VTL script | SDMX-ML representation |
| `VtlDataflowMapping` | Dataset name mapping | Links SDMX URN to VTL name |

### SDMX Functions

```python
from pysdmx.io import get_datasets
from pysdmx.model.vtl import VtlDataflowMapping, TransformationScheme
from vtlengine import run_sdmx, generate_sdmx

# Execute VTL with SDMX data
datasets = get_datasets("data.xml", "metadata.xml")
result = run_sdmx(script, datasets, mappings=mapping)

# Generate SDMX TransformationScheme from VTL
ts = generate_sdmx(script, agency_id="MD", id="TS1", version="1.0")
```

### Dataset Mapping Patterns

**Single dataset** (no mapping required):
```python
result = run_sdmx("DS_r <- DS_1 * 10;", [dataset])
# Schema ID becomes dataset name: DataStructure=MD:TEST(1.0) → TEST
```

**Multiple datasets** (mapping required):
```python
# Dictionary mapping: short_urn → VTL name
mapping = {"Dataflow=MD:TEST_DF(1.0)": "DS_1"}

# Or VtlDataflowMapping object
mapping = VtlDataflowMapping(
    dataflow="urn:sdmx:org.sdmx.infomodel.datastructure.Dataflow=MD:TEST_DF(1.0)",
    dataflow_alias="DS_1",
    id="VTL_MAP_1"
)
result = run_sdmx(script, datasets, mappings=mapping)
```

### Short URN Format

The short-URN is the meaningful part of an SDMX URN:
```
SDMX_type=Agency:ID(Version)

Examples:
  Dataflow=MD:TEST_DF(1.0)
  DataStructure=BIS:BIS_DER(1.0)
```

### Type Mapping (SDMX → VTL)

Handled by `VTL_DTYPES_MAPPING` in `src/vtlengine/Utils/__init__.py`:
- `String` → `String`
- `Integer`, `Long`, `Short` → `Integer`  
- `Float`, `Double`, `Decimal` → `Number`
- `Boolean` → `Boolean`
- `ObservationalTimePeriod`, `ReportingTimePeriod` → `TimePeriod`

### Common SDMX Errors

| Error Code | Meaning |
|------------|---------|
| `0-1-3-1` | Script expects one input, found multiple |
| `0-1-3-2` | Dataset missing Schema object |
| `0-1-3-3` | Multiple datasets without mapping |
| `0-1-3-4` | Short URN not found in mapping |
| `0-1-3-5` | Mapped dataset name not in script inputs |

## External Dependencies

- **pandas** (2.x): Primary data manipulation tool (Dataset.data is a DataFrame)
- **DuckDB** (1.4.x): Optional SQL execution engine for specific operations
- **pysdmx** (≥1.5.2): SDMX 3.0 data handling (`run_sdmx`, `generate_sdmx`)
- **sqlglot** (22.x): SQL parsing for external routines
- **antlr4-python3-runtime** (4.9.x): Parser runtime - must match grammar generation version

## Quick Reference Commands

Code quality checks (run before every commit):
```bash
poetry run ruff format src/
poetry run ruff check --fix src/
poetry run mypy src/
```

Before finishing an issue, run the full test suite (all tests must pass):
```bash
poetry run pytest tests/
```

## File Naming Conventions

- AST nodes: PascalCase dataclasses in `AST/__init__.py`
- Operators: PascalCase classes in `Operators/{Category}.py`
- Test files: `test_*.py` with snake_case functions
- VTL scripts: `{code}.vtl` in test data directories
- Data structures: `DS_{code}.json` (input) or `DS_{name}_{code}.json`
- Datapoints: `DS_{name}_{code}.csv`
